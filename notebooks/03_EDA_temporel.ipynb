{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "977087b2",
   "metadata": {},
   "source": [
    "# üìÖ Analyse EDA - Patterns Temporels\n",
    "\n",
    "Ce notebook se concentre sur l'analyse temporelle des donn√©es d'entra√Ænement.\n",
    "\n",
    "## Objectifs\n",
    "- Analyser l'√©volution des performances dans le temps\n",
    "- Identifier les patterns et tendances\n",
    "- D√©tecter les p√©riodes de progression/stagnation\n",
    "- Analyser la r√©gularit√© d'entra√Ænement\n",
    "- Pr√©parer les features temporelles pour le ML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a0b873",
   "metadata": {},
   "source": [
    "## üîß Imports et configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d918f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from datetime import datetime, timedelta\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration des graphiques\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"viridis\")\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "\n",
    "print(\"üìÖ Notebook d'analyse temporelle initialis√©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa556d9",
   "metadata": {},
   "source": [
    "## üìÅ Chargement et pr√©paration des donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4c888c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des donn√©es\n",
    "df_raw = pd.read_csv('../examples/sample_data.csv')\n",
    "\n",
    "# Nettoyage et pr√©paration\n",
    "df = df_raw.copy()\n",
    "df['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%Y')\n",
    "df['Poids_kg'] = df['Poids / Distance'].str.replace(' kg', '').str.replace(',', '.').astype(float)\n",
    "df['Reps'] = df['R√©p√©titions / Temps'].str.extract(r'(\\d+)').astype(float)\n",
    "df['Volume'] = df['Poids_kg'] * df['Reps']\n",
    "df['Type_serie'] = df['S√©rie / S√©rie d\\'√©chauffement / S√©rie de r√©cup√©ration']\n",
    "df['Sautee'] = df['Saut√©e'].map({'Oui': True, 'Non': False})\n",
    "\n",
    "# Ajout d'informations temporelles\n",
    "df['Jour_Semaine'] = df['Date'].dt.day_name()\n",
    "df['Semaine'] = df['Date'].dt.isocalendar().week\n",
    "df['Mois'] = df['Date'].dt.month\n",
    "df['Jour_Numero'] = df['Date'].dt.dayofweek  # 0=Lundi, 6=Dimanche\n",
    "\n",
    "# Tri par date pour l'analyse temporelle\n",
    "df = df.sort_values('Date')\n",
    "\n",
    "print(f\"üìä Donn√©es pr√©par√©es: {len(df)} sets\")\n",
    "print(f\"üìÖ P√©riode: du {df['Date'].min().strftime('%d/%m/%Y')} au {df['Date'].max().strftime('%d/%m/%Y')}\")\n",
    "print(f\"‚è±Ô∏è Dur√©e totale: {(df['Date'].max() - df['Date'].min()).days} jours\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ec6445",
   "metadata": {},
   "source": [
    "## üìà √âvolution globale du volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1ea294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse du volume par jour\n",
    "print(\"üìà √âVOLUTION DU VOLUME D'ENTRA√éNEMENT\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "daily_stats = df.groupby('Date').agg({\n",
    "    'Volume': ['sum', 'count', 'mean'],\n",
    "    'Poids_kg': 'mean',\n",
    "    'Reps': 'mean',\n",
    "    'Entra√Ænement': 'first',\n",
    "    'R√©gion': 'nunique'\n",
    "}).round(2)\n",
    "\n",
    "daily_stats.columns = ['Volume_Total', 'Nb_Sets', 'Volume_Moyen_Set', 'Poids_Moyen', 'Reps_Moyen', 'Type_Entrainement', 'Nb_Regions']\n",
    "daily_stats = daily_stats.reset_index()\n",
    "\n",
    "print(\"üìã Statistiques quotidiennes:\")\n",
    "print(daily_stats)\n",
    "\n",
    "# Calcul des tendances\n",
    "daily_stats['Jour_Index'] = range(len(daily_stats))\n",
    "slope_volume, intercept_volume, r_value_volume, p_value_volume, std_err_volume = stats.linregress(\n",
    "    daily_stats['Jour_Index'], daily_stats['Volume_Total']\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä TENDANCE G√âN√âRALE:\")\n",
    "print(f\"   Pente du volume: {slope_volume:.2f} kg/jour\")\n",
    "print(f\"   Corr√©lation: {r_value_volume:.3f}\")\n",
    "print(f\"   Tendance: {'üìà Croissante' if slope_volume > 0 else 'üìâ D√©croissante' if slope_volume < 0 else '‚û°Ô∏è Stable'}\")\n",
    "print(f\"   Significativit√©: {'‚úÖ Significative' if p_value_volume < 0.05 else '‚ö†Ô∏è Non significative'} (p={p_value_volume:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16da4690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation de l'√©volution du volume\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('üìà √âvolution Temporelle des M√©triques d\\'Entra√Ænement', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Volume total par jour\n",
    "axes[0,0].plot(daily_stats['Date'], daily_stats['Volume_Total'], \n",
    "               marker='o', linewidth=2, markersize=8, color='navy')\n",
    "# Ligne de tendance\n",
    "trend_line = slope_volume * daily_stats['Jour_Index'] + intercept_volume\n",
    "axes[0,0].plot(daily_stats['Date'], trend_line, '--', color='red', alpha=0.7, \n",
    "               label=f'Tendance: {slope_volume:.1f} kg/jour')\n",
    "axes[0,0].set_title('Volume Total par Jour')\n",
    "axes[0,0].set_ylabel('Volume (kg)')\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Nombre de sets par jour\n",
    "axes[0,1].bar(daily_stats['Date'], daily_stats['Nb_Sets'], color='steelblue', alpha=0.7)\n",
    "axes[0,1].set_title('Nombre de Sets par Jour')\n",
    "axes[0,1].set_ylabel('Nombre de Sets')\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Poids moyen par jour\n",
    "axes[1,0].plot(daily_stats['Date'], daily_stats['Poids_Moyen'], \n",
    "               marker='s', linewidth=2, markersize=6, color='darkgreen')\n",
    "axes[1,0].set_title('Poids Moyen par Jour')\n",
    "axes[1,0].set_ylabel('Poids Moyen (kg)')\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Volume moyen par set\n",
    "axes[1,1].plot(daily_stats['Date'], daily_stats['Volume_Moyen_Set'], \n",
    "               marker='^', linewidth=2, markersize=6, color='purple')\n",
    "axes[1,1].set_title('Volume Moyen par Set')\n",
    "axes[1,1].set_ylabel('Volume/Set (kg)')\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Formatage des axes de dates\n",
    "for ax in axes.flat:\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d589867d",
   "metadata": {},
   "source": [
    "## üèãÔ∏è Progression par exercice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20b9cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse de la progression par exercice\n",
    "print(\"üèãÔ∏è PROGRESSION PAR EXERCICE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Pour chaque exercice, analyser l'√©volution du poids et volume\n",
    "exercises = df['Exercice'].unique()\n",
    "progression_summary = []\n",
    "\n",
    "for exercise in exercises:\n",
    "    exercise_data = df[df['Exercice'] == exercise].copy()\n",
    "    exercise_data = exercise_data.sort_values('Date')\n",
    "    \n",
    "    if len(exercise_data) >= 2:  # Au moins 2 points pour calculer une progression\n",
    "        # Calcul de la progression du poids max\n",
    "        first_weight = exercise_data['Poids_kg'].iloc[0]\n",
    "        last_weight = exercise_data['Poids_kg'].iloc[-1]\n",
    "        max_weight = exercise_data['Poids_kg'].max()\n",
    "        \n",
    "        # Calcul de la progression du volume\n",
    "        first_volume = exercise_data['Volume'].iloc[0]\n",
    "        last_volume = exercise_data['Volume'].iloc[-1]\n",
    "        avg_volume = exercise_data['Volume'].mean()\n",
    "        \n",
    "        # R√©gression lin√©aire pour la tendance\n",
    "        days = [(date - exercise_data['Date'].iloc[0]).days for date in exercise_data['Date']]\n",
    "        if len(set(exercise_data['Poids_kg'])) > 1:  # Variation de poids\n",
    "            slope_weight, _, r_weight, p_weight, _ = stats.linregress(days, exercise_data['Poids_kg'])\n",
    "        else:\n",
    "            slope_weight, r_weight, p_weight = 0, 0, 1\n",
    "        \n",
    "        progression_summary.append({\n",
    "            'Exercice': exercise,\n",
    "            'Nb_Sessions': len(exercise_data),\n",
    "            'Premier_Poids': first_weight,\n",
    "            'Dernier_Poids': last_weight,\n",
    "            'Poids_Max': max_weight,\n",
    "            'Progression_Poids': last_weight - first_weight,\n",
    "            'Progression_Pct': ((last_weight - first_weight) / first_weight * 100) if first_weight > 0 else 0,\n",
    "            'Tendance_Poids_Jour': slope_weight,\n",
    "            'Correlation_Temps': r_weight,\n",
    "            'Volume_Moyen': avg_volume,\n",
    "            'Progression_Volume': last_volume - first_volume\n",
    "        })\n",
    "\n",
    "progression_df = pd.DataFrame(progression_summary)\n",
    "progression_df = progression_df.round(2).sort_values('Progression_Pct', ascending=False)\n",
    "\n",
    "print(\"üìä R√©sum√© des progressions:\")\n",
    "print(progression_df)\n",
    "\n",
    "# Identification des meilleures et moins bonnes progressions\n",
    "best_progression = progression_df.iloc[0]\n",
    "worst_progression = progression_df.iloc[-1]\n",
    "\n",
    "print(f\"\\nüèÜ MEILLEURE PROGRESSION:\")\n",
    "print(f\"   Exercice: {best_progression['Exercice']}\")\n",
    "print(f\"   Progression: +{best_progression['Progression_Poids']:.1f}kg ({best_progression['Progression_Pct']:.1f}%)\")\n",
    "print(f\"   Tendance: {best_progression['Tendance_Poids_Jour']:.3f} kg/jour\")\n",
    "\n",
    "print(f\"\\nüìâ PROGRESSION √Ä AM√âLIORER:\")\n",
    "print(f\"   Exercice: {worst_progression['Exercice']}\")\n",
    "print(f\"   Progression: {worst_progression['Progression_Poids']:.1f}kg ({worst_progression['Progression_Pct']:.1f}%)\")\n",
    "print(f\"   Tendance: {worst_progression['Tendance_Poids_Jour']:.3f} kg/jour\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67104ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation de la progression par exercice\n",
    "fig, axes = plt.subplots(len(exercises), 1, figsize=(14, 6*len(exercises)))\n",
    "if len(exercises) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "fig.suptitle('üèãÔ∏è Progression D√©taill√©e par Exercice', fontsize=16, fontweight='bold')\n",
    "\n",
    "for i, exercise in enumerate(exercises):\n",
    "    exercise_data = df[df['Exercice'] == exercise].sort_values('Date')\n",
    "    \n",
    "    # Graphique principal : poids dans le temps\n",
    "    ax = axes[i]\n",
    "    scatter = ax.scatter(exercise_data['Date'], exercise_data['Poids_kg'], \n",
    "                        c=exercise_data['Volume'], cmap='viridis', \n",
    "                        s=exercise_data['Reps']*10, alpha=0.7)\n",
    "    \n",
    "    # Ligne de tendance si donn√©es suffisantes\n",
    "    if len(exercise_data) >= 2:\n",
    "        z = np.polyfit(range(len(exercise_data)), exercise_data['Poids_kg'], 1)\n",
    "        p = np.poly1d(z)\n",
    "        ax.plot(exercise_data['Date'], p(range(len(exercise_data))), \"--\", \n",
    "               color='red', alpha=0.8, linewidth=2)\n",
    "    \n",
    "    ax.set_title(f'{exercise} - √âvolution du Poids (couleur=volume, taille=reps)')\n",
    "    ax.set_ylabel('Poids (kg)')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Colorbar pour le volume\n",
    "    cbar = plt.colorbar(scatter, ax=ax)\n",
    "    cbar.set_label('Volume (kg)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4389dcdb",
   "metadata": {},
   "source": [
    "## üìä Analyse des patterns hebdomadaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93510f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse des patterns hebdomadaires\n",
    "print(\"üìä ANALYSE DES PATTERNS HEBDOMADAIRES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# R√©partition par jour de la semaine\n",
    "weekly_patterns = df.groupby(['Jour_Semaine', 'Jour_Numero']).agg({\n",
    "    'Volume': ['sum', 'count', 'mean'],\n",
    "    'Date': 'nunique'\n",
    "}).round(2)\n",
    "\n",
    "weekly_patterns.columns = ['Volume_Total', 'Nb_Sets', 'Volume_Moyen', 'Nb_Jours_Entrainement']\n",
    "weekly_patterns = weekly_patterns.reset_index().sort_values('Jour_Numero')\n",
    "\n",
    "print(\"üìÖ Activit√© par jour de la semaine:\")\n",
    "print(weekly_patterns)\n",
    "\n",
    "# Calcul de l'intensit√© par jour (volume/nombre de jours d'entra√Ænement)\n",
    "weekly_patterns['Intensite_Jour'] = weekly_patterns['Volume_Total'] / weekly_patterns['Nb_Jours_Entrainement']\n",
    "\n",
    "print(f\"\\nüèÜ Jour le plus actif: {weekly_patterns.loc[weekly_patterns['Volume_Total'].idxmax(), 'Jour_Semaine']}\")\n",
    "print(f\"üò¥ Jour le moins actif: {weekly_patterns.loc[weekly_patterns['Volume_Total'].idxmin(), 'Jour_Semaine']}\")\n",
    "print(f\"üí™ Jour le plus intensif: {weekly_patterns.loc[weekly_patterns['Intensite_Jour'].idxmax(), 'Jour_Semaine']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1321164a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des patterns hebdomadaires\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "fig.suptitle('üìÖ Patterns Hebdomadaires d\\'Entra√Ænement', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Volume total par jour de la semaine\n",
    "axes[0,0].bar(weekly_patterns['Jour_Semaine'], weekly_patterns['Volume_Total'], \n",
    "              color='skyblue', alpha=0.8)\n",
    "axes[0,0].set_title('Volume Total par Jour de la Semaine')\n",
    "axes[0,0].set_ylabel('Volume Total (kg)')\n",
    "axes[0,0].tick_params(axis='x', rotation=45)\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Nombre de sets par jour\n",
    "axes[0,1].bar(weekly_patterns['Jour_Semaine'], weekly_patterns['Nb_Sets'], \n",
    "              color='lightgreen', alpha=0.8)\n",
    "axes[0,1].set_title('Nombre de Sets par Jour de la Semaine')\n",
    "axes[0,1].set_ylabel('Nombre de Sets')\n",
    "axes[0,1].tick_params(axis='x', rotation=45)\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Intensit√© (volume/jour d'entra√Ænement)\n",
    "axes[1,0].bar(weekly_patterns['Jour_Semaine'], weekly_patterns['Intensite_Jour'], \n",
    "              color='orange', alpha=0.8)\n",
    "axes[1,0].set_title('Intensit√© par Jour (Volume/Jour d\\'entra√Ænement)')\n",
    "axes[1,0].set_ylabel('Intensit√© (kg)')\n",
    "axes[1,0].tick_params(axis='x', rotation=45)\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Heatmap des r√©gions par jour de la semaine\n",
    "region_day_pivot = df.groupby(['Jour_Semaine', 'R√©gion'])['Volume'].sum().unstack(fill_value=0)\n",
    "sns.heatmap(region_day_pivot.T, annot=True, fmt='.0f', cmap='YlOrRd', ax=axes[1,1])\n",
    "axes[1,1].set_title('Volume par R√©gion et Jour (Heatmap)')\n",
    "axes[1,1].set_xlabel('Jour de la Semaine')\n",
    "axes[1,1].set_ylabel('R√©gion Musculaire')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40f1098",
   "metadata": {},
   "source": [
    "## ‚è±Ô∏è Analyse de la r√©gularit√© d'entra√Ænement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985f10bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse de la r√©gularit√©\n",
    "print(\"‚è±Ô∏è ANALYSE DE LA R√âGULARIT√â D'ENTRA√éNEMENT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Calcul des intervalles entre sessions\n",
    "training_dates = df['Date'].unique()\n",
    "training_dates.sort()\n",
    "\n",
    "intervals = []\n",
    "for i in range(1, len(training_dates)):\n",
    "    interval = (training_dates[i] - training_dates[i-1]).days\n",
    "    intervals.append(interval)\n",
    "\n",
    "if intervals:\n",
    "    intervals_df = pd.DataFrame({\n",
    "        'Date_Debut': training_dates[:-1],\n",
    "        'Date_Fin': training_dates[1:],\n",
    "        'Intervalle_Jours': intervals\n",
    "    })\n",
    "    \n",
    "    print(\"üìä Statistiques des intervalles entre sessions:\")\n",
    "    print(f\"   Intervalle moyen: {np.mean(intervals):.1f} jours\")\n",
    "    print(f\"   Intervalle m√©dian: {np.median(intervals):.1f} jours\")\n",
    "    print(f\"   √âcart-type: {np.std(intervals):.1f} jours\")\n",
    "    print(f\"   Intervalle min: {min(intervals)} jours\")\n",
    "    print(f\"   Intervalle max: {max(intervals)} jours\")\n",
    "    \n",
    "    # Classification de la r√©gularit√©\n",
    "    if np.std(intervals) <= 1:\n",
    "        regularity = \"üéØ Tr√®s r√©guli√®re\"\n",
    "    elif np.std(intervals) <= 2:\n",
    "        regularity = \"‚úÖ R√©guli√®re\"\n",
    "    elif np.std(intervals) <= 3:\n",
    "        regularity = \"‚ö†Ô∏è Mod√©r√©ment irr√©guli√®re\"\n",
    "    else:\n",
    "        regularity = \"‚ùå Irr√©guli√®re\"\n",
    "    \n",
    "    print(f\"\\n‚è±Ô∏è √âvaluation de la r√©gularit√©: {regularity}\")\n",
    "    \n",
    "    print(\"\\nüìÖ D√©tail des intervalles:\")\n",
    "    print(intervals_df)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Pas assez de donn√©es pour analyser la r√©gularit√©\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73928ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation de la r√©gularit√©\n",
    "if intervals:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # 1. Histogramme des intervalles\n",
    "    axes[0].hist(intervals, bins=max(1, len(set(intervals))), alpha=0.7, color='steelblue', edgecolor='black')\n",
    "    axes[0].axvline(np.mean(intervals), color='red', linestyle='--', \n",
    "                   label=f'Moyenne: {np.mean(intervals):.1f} jours')\n",
    "    axes[0].set_title('üìä Distribution des Intervalles entre Sessions')\n",
    "    axes[0].set_xlabel('Intervalle (jours)')\n",
    "    axes[0].set_ylabel('Fr√©quence')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. √âvolution des intervalles dans le temps\n",
    "    axes[1].plot(range(len(intervals)), intervals, marker='o', linewidth=2, markersize=6)\n",
    "    axes[1].axhline(np.mean(intervals), color='red', linestyle='--', alpha=0.7, \n",
    "                   label=f'Moyenne: {np.mean(intervals):.1f} jours')\n",
    "    axes[1].set_title('‚è±Ô∏è √âvolution des Intervalles dans le Temps')\n",
    "    axes[1].set_xlabel('Num√©ro de la Pause')\n",
    "    axes[1].set_ylabel('Intervalle (jours)')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Pas assez de donn√©es pour visualiser la r√©gularit√©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312b288e",
   "metadata": {},
   "source": [
    "## üé≤ Calcul des features temporelles pour ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b689b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul des features temporelles avanc√©es\n",
    "print(\"üé≤ CALCUL DES FEATURES TEMPORELLES POUR ML\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Cr√©ation d'un DataFrame enrichi avec des features temporelles\n",
    "df_features = df.copy()\n",
    "\n",
    "# 1. Features bas√©es sur l'ordre chronologique\n",
    "df_features = df_features.sort_values(['Exercice', 'Date'])\n",
    "df_features['Session_Number'] = df_features.groupby('Exercice').cumcount() + 1\n",
    "\n",
    "# 2. Rolling windows pour tendances\n",
    "df_features['Volume_Rolling_3'] = df_features.groupby('Exercice')['Volume'].rolling(window=3, min_periods=1).mean().reset_index(0, drop=True)\n",
    "df_features['Poids_Rolling_3'] = df_features.groupby('Exercice')['Poids_kg'].rolling(window=3, min_periods=1).mean().reset_index(0, drop=True)\n",
    "\n",
    "# 3. Features de progression\n",
    "df_features['Poids_Progression'] = df_features.groupby('Exercice')['Poids_kg'].diff()\n",
    "df_features['Volume_Progression'] = df_features.groupby('Exercice')['Volume'].diff()\n",
    "\n",
    "# 4. Features relatives au maximum personnel\n",
    "df_features['Poids_Max_Personnel'] = df_features.groupby('Exercice')['Poids_kg'].cummax()\n",
    "df_features['Pct_Max_Personnel'] = (df_features['Poids_kg'] / df_features['Poids_Max_Personnel'] * 100).round(1)\n",
    "\n",
    "# 5. Features temporelles cycliques\n",
    "df_features['Jour_Semaine_Sin'] = np.sin(2 * np.pi * df_features['Jour_Numero'] / 7)\n",
    "df_features['Jour_Semaine_Cos'] = np.cos(2 * np.pi * df_features['Jour_Numero'] / 7)\n",
    "\n",
    "# 6. D√©lai depuis la derni√®re session de l'exercice\n",
    "df_features['Jours_Depuis_Dernier'] = df_features.groupby('Exercice')['Date'].diff().dt.days\n",
    "\n",
    "# 7. Volume cumul√©\n",
    "df_features['Volume_Cumule'] = df_features.groupby('Exercice')['Volume'].cumsum()\n",
    "\n",
    "print(\"‚úÖ Features temporelles calcul√©es:\")\n",
    "new_features = ['Session_Number', 'Volume_Rolling_3', 'Poids_Rolling_3', \n",
    "                'Poids_Progression', 'Volume_Progression', 'Poids_Max_Personnel',\n",
    "                'Pct_Max_Personnel', 'Jour_Semaine_Sin', 'Jour_Semaine_Cos',\n",
    "                'Jours_Depuis_Dernier', 'Volume_Cumule']\n",
    "\n",
    "for feature in new_features:\n",
    "    print(f\"   ‚Ä¢ {feature}\")\n",
    "\n",
    "# Aper√ßu des nouvelles features\n",
    "print(\"\\nüìä Aper√ßu des features temporelles:\")\n",
    "feature_sample = df_features[['Date', 'Exercice', 'Poids_kg', 'Volume'] + new_features].head(10)\n",
    "print(feature_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6181af26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse de corr√©lation des nouvelles features\n",
    "print(\"üîç ANALYSE DE CORR√âLATION DES FEATURES TEMPORELLES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# S√©lection des features num√©riques pour l'analyse de corr√©lation\n",
    "numeric_features = ['Poids_kg', 'Reps', 'Volume', 'Session_Number', \n",
    "                   'Volume_Rolling_3', 'Poids_Rolling_3', 'Poids_Progression',\n",
    "                   'Volume_Progression', 'Pct_Max_Personnel', 'Volume_Cumule']\n",
    "\n",
    "correlation_matrix = df_features[numeric_features].corr()\n",
    "\n",
    "# Visualisation de la matrice de corr√©lation\n",
    "plt.figure(figsize=(12, 10))\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='coolwarm', center=0,\n",
    "            square=True, fmt='.2f')\n",
    "plt.title('üîç Matrice de Corr√©lation des Features Temporelles', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identification des corr√©lations fortes\n",
    "print(\"üîó Corr√©lations significatives (|r| > 0.7):\")\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        corr_value = correlation_matrix.iloc[i, j]\n",
    "        if abs(corr_value) > 0.7:\n",
    "            feature1 = correlation_matrix.columns[i]\n",
    "            feature2 = correlation_matrix.columns[j]\n",
    "            print(f\"   ‚Ä¢ {feature1} ‚Üî {feature2}: {corr_value:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ad1e85",
   "metadata": {},
   "source": [
    "## üéØ R√©sum√© et insights temporels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497d8398",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéØ R√âSUM√â ET INSIGHTS TEMPORELS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# M√©triques temporelles globales\n",
    "total_period = (df['Date'].max() - df['Date'].min()).days\n",
    "training_days = df['Date'].nunique()\n",
    "training_frequency = training_days / (total_period + 1) * 7 if total_period > 0 else 0\n",
    "\n",
    "print(f\"üìä M√âTRIQUES TEMPORELLES GLOBALES:\")\n",
    "print(f\"   ‚Ä¢ P√©riode totale: {total_period} jours\")\n",
    "print(f\"   ‚Ä¢ Jours d'entra√Ænement: {training_days}\")\n",
    "print(f\"   ‚Ä¢ Fr√©quence hebdomadaire: {training_frequency:.1f} jours/semaine\")\n",
    "print(f\"   ‚Ä¢ R√©gularit√©: {regularity if 'regularity' in locals() else 'Non calcul√©e'}\")\n",
    "\n",
    "print(f\"\\nüìà TENDANCES DE PROGRESSION:\")\n",
    "total_exercises_with_progression = len(progression_df[progression_df['Progression_Poids'] > 0]) if 'progression_df' in locals() else 0\n",
    "total_exercises = len(progression_df) if 'progression_df' in locals() else 0\n",
    "\n",
    "if total_exercises > 0:\n",
    "    progression_rate = total_exercises_with_progression / total_exercises * 100\n",
    "    print(f\"   ‚Ä¢ Exercices en progression: {total_exercises_with_progression}/{total_exercises} ({progression_rate:.1f}%)\")\n",
    "    print(f\"   ‚Ä¢ Progression moyenne: {progression_df['Progression_Poids'].mean():.2f} kg\")\n",
    "    print(f\"   ‚Ä¢ Meilleur exercice: {best_progression['Exercice']} (+{best_progression['Progression_Pct']:.1f}%)\")\n",
    "\n",
    "print(f\"\\nüìÖ PATTERNS HEBDOMADAIRES:\")\n",
    "if 'weekly_patterns' in locals() and len(weekly_patterns) > 0:\n",
    "    best_day = weekly_patterns.loc[weekly_patterns['Volume_Total'].idxmax(), 'Jour_Semaine']\n",
    "    best_day_volume = weekly_patterns['Volume_Total'].max()\n",
    "    print(f\"   ‚Ä¢ Jour le plus productif: {best_day} ({best_day_volume:.0f}kg)\")\n",
    "    print(f\"   ‚Ä¢ R√©partition √©quilibr√©e: {'‚úÖ Oui' if weekly_patterns['Volume_Total'].std() < weekly_patterns['Volume_Total'].mean() * 0.5 else '‚ö†Ô∏è √Ä am√©liorer'}\")\n",
    "\n",
    "print(f\"\\nüé≤ FEATURES ML G√âN√âR√âES:\")\n",
    "print(f\"   ‚Ä¢ {len(new_features)} nouvelles features temporelles\")\n",
    "print(f\"   ‚Ä¢ Rolling windows pour tendances\")\n",
    "print(f\"   ‚Ä¢ Features cycliques pour saisonnalit√©\")\n",
    "print(f\"   ‚Ä¢ M√©triques de progression individuelles\")\n",
    "\n",
    "print(f\"\\nüöÄ RECOMMANDATIONS TEMPORELLES:\")\n",
    "if training_frequency < 3:\n",
    "    print(f\"   ‚ö†Ô∏è Fr√©quence d'entra√Ænement faible - augmenter √† 3-4 fois/semaine\")\n",
    "elif training_frequency > 6:\n",
    "    print(f\"   ‚ö†Ô∏è Fr√©quence tr√®s √©lev√©e - pr√©voir des jours de repos\")\n",
    "else:\n",
    "    print(f\"   ‚úÖ Fr√©quence d'entra√Ænement optimale\")\n",
    "\n",
    "if 'regularity' in locals() and '‚ùå' in regularity:\n",
    "    print(f\"   üìÖ Am√©liorer la r√©gularit√© des sessions\")\n",
    "elif 'regularity' in locals() and 'üéØ' in regularity:\n",
    "    print(f\"   ‚úÖ Excellente r√©gularit√© maintenue\")\n",
    "\n",
    "if total_exercises > 0 and progression_rate < 50:\n",
    "    print(f\"   üìà Revoir la programmation - moins de 50% des exercices progressent\")\n",
    "elif total_exercises > 0 and progression_rate > 80:\n",
    "    print(f\"   üèÜ Excellente progression g√©n√©rale\")\n",
    "\n",
    "print(f\"\\nüìö PROCHAINES ANALYSES RECOMMAND√âES:\")\n",
    "print(f\"   1. Mod√©lisation pr√©dictive avec les features temporelles\")\n",
    "print(f\"   2. D√©tection de plateaux automatis√©e\")\n",
    "print(f\"   3. Recommandations de charge optimale\")\n",
    "print(f\"   4. Analyse de saisonnalit√© sur plus de donn√©es\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d089d8",
   "metadata": {},
   "source": [
    "---\n",
    "## üìù R√©sum√© de l'analyse temporelle\n",
    "\n",
    "Ce notebook a fourni une analyse temporelle compl√®te des donn√©es d'entra√Ænement :\n",
    "\n",
    "### ‚úÖ Analyses r√©alis√©es\n",
    "- **√âvolution globale** : Tendances de volume et progression\n",
    "- **Progression par exercice** : Tracking individuel des performances\n",
    "- **Patterns hebdomadaires** : Identification des jours optimaux\n",
    "- **R√©gularit√©** : Analyse de la constance d'entra√Ænement\n",
    "- **Features ML** : 11 nouvelles variables temporelles\n",
    "\n",
    "### üéØ Insights cl√©s\n",
    "- Tendances de progression identifi√©es\n",
    "- Patterns comportementaux r√©v√©l√©s\n",
    "- Features pr√™tes pour mod√©lisation ML\n",
    "- Recommandations d'optimisation\n",
    "\n",
    "### üìä Donn√©es g√©n√©r√©es\n",
    "- `df_features`: Dataset enrichi avec features temporelles\n",
    "- `progression_df`: M√©triques de progression par exercice\n",
    "- `weekly_patterns`: Analyse hebdomadaire\n",
    "- `daily_stats`: Statistiques quotidiennes\n",
    "\n",
    "**Prochaine √©tape:** Feature engineering avanc√© et pr√©paration pour les mod√®les ML (04_features_engineering.ipynb)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
