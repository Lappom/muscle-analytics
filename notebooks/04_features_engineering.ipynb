{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50774263",
   "metadata": {},
   "source": [
    "# üîß Feature Engineering Avanc√©\n",
    "\n",
    "Ce notebook se concentre sur la cr√©ation de features avanc√©es pour l'analyse et la mod√©lisation ML.\n",
    "\n",
    "## Objectifs\n",
    "- Calculer les m√©triques avanc√©es (1RM, volume cumul√©, etc.)\n",
    "- Cr√©er des indicateurs de progression\n",
    "- Pr√©parer les features pour les mod√®les ML\n",
    "- Analyser la distribution et qualit√© des nouvelles features\n",
    "- Identifier les features les plus importantes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25121acf",
   "metadata": {},
   "source": [
    "## üîß Imports et configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42427312",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "plt.style.use('seaborn')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "\n",
    "print(\"üîß Feature Engineering - Environnement initialis√©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af72433d",
   "metadata": {},
   "source": [
    "## üìÅ Chargement et pr√©paration des donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b143db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des donn√©es\n",
    "df_raw = pd.read_csv('../examples/sample_data.csv')\n",
    "\n",
    "# Pr√©paration compl√®te (reprise des notebooks pr√©c√©dents)\n",
    "df = df_raw.copy()\n",
    "df['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%Y')\n",
    "df['Poids_kg'] = df['Poids / Distance'].str.replace(' kg', '').str.replace(',', '.').astype(float)\n",
    "df['Reps'] = df['R√©p√©titions / Temps'].str.extract(r'(\\d+)').astype(float)\n",
    "df['Volume'] = df['Poids_kg'] * df['Reps']\n",
    "df['Type_serie'] = df['S√©rie / S√©rie d\\'√©chauffement / S√©rie de r√©cup√©ration']\n",
    "df['Sautee'] = df['Saut√©e'].map({'Oui': True, 'Non': False})\n",
    "\n",
    "# Informations temporelles\n",
    "df['Jour_Numero'] = df['Date'].dt.dayofweek\n",
    "df['Semaine'] = df['Date'].dt.isocalendar().week\n",
    "df['Mois'] = df['Date'].dt.month\n",
    "\n",
    "# Tri par date et exercice\n",
    "df = df.sort_values(['Exercice', 'Date']).reset_index(drop=True)\n",
    "\n",
    "print(f\"üîß Donn√©es pr√©par√©es: {len(df)} sets\")\n",
    "print(f\"üìä Exercices uniques: {df['Exercice'].nunique()}\")\n",
    "print(f\"üìÖ P√©riode: {df['Date'].min().strftime('%d/%m/%Y')} ‚Üí {df['Date'].max().strftime('%d/%m/%Y')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717664cb",
   "metadata": {},
   "source": [
    "## üí™ Calcul du 1RM (One Rep Max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42c5e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul du 1RM avec diff√©rentes formules\n",
    "print(\"üí™ CALCUL DU 1RM (ONE REP MAX)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "def calculate_1rm_epley(weight, reps):\n",
    "    \"\"\"Formule d'Epley: 1RM = poids √ó (1 + reps/30)\"\"\"\n",
    "    if reps <= 0 or weight <= 0:\n",
    "        return np.nan\n",
    "    return weight * (1 + reps / 30)\n",
    "\n",
    "def calculate_1rm_brzycki(weight, reps):\n",
    "    \"\"\"Formule de Brzycki: 1RM = poids √ó 36 / (37 - reps)\"\"\"\n",
    "    if reps <= 0 or reps >= 37 or weight <= 0:\n",
    "        return np.nan\n",
    "    return weight * 36 / (37 - reps)\n",
    "\n",
    "def calculate_1rm_lombardi(weight, reps):\n",
    "    \"\"\"Formule de Lombardi: 1RM = poids √ó reps^0.10\"\"\"\n",
    "    if reps <= 0 or weight <= 0:\n",
    "        return np.nan\n",
    "    return weight * (reps ** 0.10)\n",
    "\n",
    "# Application des formules\n",
    "df['1RM_Epley'] = df.apply(lambda row: calculate_1rm_epley(row['Poids_kg'], row['Reps']), axis=1)\n",
    "df['1RM_Brzycki'] = df.apply(lambda row: calculate_1rm_brzycki(row['Poids_kg'], row['Reps']), axis=1)\n",
    "df['1RM_Lombardi'] = df.apply(lambda row: calculate_1rm_lombardi(row['Poids_kg'], row['Reps']), axis=1)\n",
    "\n",
    "# 1RM moyen (moyenne des 3 formules)\n",
    "df['1RM_Moyen'] = df[['1RM_Epley', '1RM_Brzycki', '1RM_Lombardi']].mean(axis=1)\n",
    "\n",
    "# Filtrer les s√©ries principales pour le 1RM (exclure √©chauffement)\n",
    "df_main_sets = df[df['Type_serie'] == 'S√©rie'].copy()\n",
    "\n",
    "print(\"üìä Aper√ßu des calculs 1RM:\")\n",
    "oneRM_sample = df_main_sets[['Date', 'Exercice', 'Poids_kg', 'Reps', '1RM_Epley', '1RM_Brzycki', '1RM_Lombardi', '1RM_Moyen']].head(10)\n",
    "print(oneRM_sample)\n",
    "\n",
    "print(f\"\\nüìà Statistiques 1RM (s√©ries principales uniquement):\")\n",
    "oneRM_stats = df_main_sets[['1RM_Epley', '1RM_Brzycki', '1RM_Lombardi', '1RM_Moyen']].describe()\n",
    "print(oneRM_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a47b1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des 1RM par exercice\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('üí™ Analyse des 1RM par Exercice', fontsize=16, fontweight='bold')\n",
    "\n",
    "exercises = df_main_sets['Exercice'].unique()\n",
    "\n",
    "# 1. Comparaison des formules de 1RM\n",
    "oneRM_comparison = df_main_sets.groupby('Exercice')[['1RM_Epley', '1RM_Brzycki', '1RM_Lombardi']].mean()\n",
    "oneRM_comparison.plot(kind='bar', ax=axes[0,0], alpha=0.8)\n",
    "axes[0,0].set_title('Comparaison des Formules 1RM par Exercice')\n",
    "axes[0,0].set_ylabel('1RM Moyen (kg)')\n",
    "axes[0,0].tick_params(axis='x', rotation=45)\n",
    "axes[0,0].legend()\n",
    "\n",
    "# 2. √âvolution du 1RM dans le temps (premier exercice)\n",
    "if len(exercises) > 0:\n",
    "    main_exercise = exercises[0]\n",
    "    exercise_data = df_main_sets[df_main_sets['Exercice'] == main_exercise]\n",
    "    axes[0,1].plot(exercise_data['Date'], exercise_data['1RM_Moyen'], \n",
    "                   marker='o', linewidth=2, markersize=6)\n",
    "    axes[0,1].set_title(f'√âvolution 1RM - {main_exercise}')\n",
    "    axes[0,1].set_ylabel('1RM (kg)')\n",
    "    axes[0,1].tick_params(axis='x', rotation=45)\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Distribution des 1RM\n",
    "df_main_sets['1RM_Moyen'].hist(bins=15, ax=axes[1,0], alpha=0.7, color='skyblue', edgecolor='black')\n",
    "axes[1,0].set_title('Distribution des 1RM Moyens')\n",
    "axes[1,0].set_xlabel('1RM (kg)')\n",
    "axes[1,0].set_ylabel('Fr√©quence')\n",
    "axes[1,0].axvline(df_main_sets['1RM_Moyen'].mean(), color='red', linestyle='--', \n",
    "                 label=f'Moyenne: {df_main_sets[\"1RM_Moyen\"].mean():.1f}kg')\n",
    "axes[1,0].legend()\n",
    "\n",
    "# 4. Boxplot 1RM par exercice\n",
    "if len(exercises) > 1:\n",
    "    df_main_sets.boxplot(column='1RM_Moyen', by='Exercice', ax=axes[1,1])\n",
    "    axes[1,1].set_title('Distribution 1RM par Exercice')\n",
    "    axes[1,1].set_xlabel('Exercice')\n",
    "    axes[1,1].set_ylabel('1RM (kg)')\n",
    "else:\n",
    "    axes[1,1].text(0.5, 0.5, 'Pas assez d\\'exercices\\npour comparaison', \n",
    "                   ha='center', va='center', transform=axes[1,1].transAxes)\n",
    "    axes[1,1].set_title('Distribution 1RM par Exercice')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fd8444",
   "metadata": {},
   "source": [
    "## üìà Features de progression et tendances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d41f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul des features de progression\n",
    "print(\"üìà FEATURES DE PROGRESSION ET TENDANCES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. Num√©rotation des sessions par exercice\n",
    "df['Session_Number'] = df.groupby('Exercice').cumcount() + 1\n",
    "\n",
    "# 2. Maximum personnel atteint (rolling max)\n",
    "df['Max_Poids_Personnel'] = df.groupby('Exercice')['Poids_kg'].cummax()\n",
    "df['Max_1RM_Personnel'] = df.groupby('Exercice')['1RM_Moyen'].cummax()\n",
    "df['Max_Volume_Personnel'] = df.groupby('Exercice')['Volume'].cummax()\n",
    "\n",
    "# 3. Pourcentage du maximum personnel\n",
    "df['Pct_Max_Poids'] = (df['Poids_kg'] / df['Max_Poids_Personnel'] * 100).round(1)\n",
    "df['Pct_Max_1RM'] = (df['1RM_Moyen'] / df['Max_1RM_Personnel'] * 100).round(1)\n",
    "\n",
    "# 4. Progressions (diff√©rences par rapport √† la session pr√©c√©dente)\n",
    "df['Progression_Poids'] = df.groupby('Exercice')['Poids_kg'].diff()\n",
    "df['Progression_Volume'] = df.groupby('Exercice')['Volume'].diff()\n",
    "df['Progression_1RM'] = df.groupby('Exercice')['1RM_Moyen'].diff()\n",
    "\n",
    "# 5. Rolling averages (moyennes mobiles)\n",
    "for window in [3, 5]:\n",
    "    df[f'Poids_Rolling_{window}'] = df.groupby('Exercice')['Poids_kg'].rolling(window=window, min_periods=1).mean().reset_index(0, drop=True)\n",
    "    df[f'Volume_Rolling_{window}'] = df.groupby('Exercice')['Volume'].rolling(window=window, min_periods=1).mean().reset_index(0, drop=True)\n",
    "    df[f'1RM_Rolling_{window}'] = df.groupby('Exercice')['1RM_Moyen'].rolling(window=window, min_periods=1).mean().reset_index(0, drop=True)\n",
    "\n",
    "# 6. Tendances (r√©gression lin√©aire sur les N derni√®res sessions)\n",
    "from scipy import stats\n",
    "\n",
    "def calculate_trend(group, column, window=5):\n",
    "    \"\"\"Calcule la tendance (pente) sur les derni√®res sessions\"\"\"\n",
    "    trends = []\n",
    "    for i in range(len(group)):\n",
    "        start_idx = max(0, i - window + 1)\n",
    "        end_idx = i + 1\n",
    "        \n",
    "        if end_idx - start_idx >= 2:\n",
    "            x_vals = list(range(end_idx - start_idx))\n",
    "            y_vals = group[column].iloc[start_idx:end_idx].values\n",
    "            \n",
    "            if len(y_vals) >= 2 and not np.isnan(y_vals).all():\n",
    "                slope, _, _, _, _ = stats.linregress(x_vals, y_vals)\n",
    "                trends.append(slope)\n",
    "            else:\n",
    "                trends.append(0)\n",
    "        else:\n",
    "            trends.append(0)\n",
    "    \n",
    "    return trends\n",
    "\n",
    "# Application du calcul de tendance\n",
    "df['Tendance_Poids_5'] = df.groupby('Exercice').apply(lambda x: pd.Series(calculate_trend(x, 'Poids_kg', 5), index=x.index)).reset_index(0, drop=True)\n",
    "df['Tendance_Volume_5'] = df.groupby('Exercice').apply(lambda x: pd.Series(calculate_trend(x, 'Volume', 5), index=x.index)).reset_index(0, drop=True)\n",
    "\n",
    "print(\"‚úÖ Features de progression calcul√©es:\")\n",
    "progression_features = ['Session_Number', 'Max_Poids_Personnel', 'Max_1RM_Personnel', \n",
    "                       'Pct_Max_Poids', 'Pct_Max_1RM', 'Progression_Poids', \n",
    "                       'Progression_Volume', 'Poids_Rolling_3', 'Volume_Rolling_3',\n",
    "                       'Tendance_Poids_5', 'Tendance_Volume_5']\n",
    "\n",
    "for feature in progression_features:\n",
    "    print(f\"   ‚Ä¢ {feature}\")\n",
    "\n",
    "print(f\"\\nüìä Aper√ßu des features de progression:\")\n",
    "progression_sample = df[['Date', 'Exercice', 'Poids_kg', 'Volume'] + progression_features[:6]].head(8)\n",
    "print(progression_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822e6841",
   "metadata": {},
   "source": [
    "## üéØ Features de performance et intensit√©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b204929c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul des features de performance\n",
    "print(\"üéØ FEATURES DE PERFORMANCE ET INTENSIT√â\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. Intensit√© relative (% du 1RM)\n",
    "df['Intensite_Relative'] = (df['Poids_kg'] / df['1RM_Moyen'] * 100).round(1)\n",
    "\n",
    "# 2. Volume par kg de poids corporel (simul√© √† 80kg)\n",
    "poids_corporel_simule = 80  # kg\n",
    "df['Volume_Par_Kg_Corps'] = (df['Volume'] / poids_corporel_simule).round(2)\n",
    "\n",
    "# 3. Densit√© d'entra√Ænement (volume par minute - simul√©)\n",
    "# Simulation: 3-5 minutes par set\n",
    "df['Duree_Set_Estimee'] = np.random.uniform(3, 5, len(df))  # minutes\n",
    "df['Densite_Volume'] = (df['Volume'] / df['Duree_Set_Estimee']).round(2)\n",
    "\n",
    "# 4. Fatigue index (d√©gradation dans la session)\n",
    "def calculate_fatigue_index(group):\n",
    "    \"\"\"Calcule l'index de fatigue pour une session\"\"\"\n",
    "    if len(group) < 2:\n",
    "        return [0] * len(group)\n",
    "    \n",
    "    # Fatigue = % de perte par rapport au premier set de la session\n",
    "    first_weight = group['Poids_kg'].iloc[0]\n",
    "    fatigue = [(first_weight - weight) / first_weight * 100 if first_weight > 0 else 0 \n",
    "               for weight in group['Poids_kg']]\n",
    "    return fatigue\n",
    "\n",
    "df['Fatigue_Index'] = df.groupby(['Date', 'Exercice']).apply(lambda x: pd.Series(calculate_fatigue_index(x), index=x.index)).reset_index(0, drop=True)\n",
    "\n",
    "# 5. Efficacit√© (volume par r√©p√©tition)\n",
    "df['Efficacite_Volume'] = (df['Volume'] / df['Reps']).round(2)\n",
    "\n",
    "# 6. Consistance (√©cart par rapport √† la moyenne personnelle)\n",
    "df['Moyenne_Poids_Perso'] = df.groupby('Exercice')['Poids_kg'].expanding().mean().reset_index(0, drop=True)\n",
    "df['Ecart_Moyenne_Perso'] = ((df['Poids_kg'] - df['Moyenne_Poids_Perso']) / df['Moyenne_Poids_Perso'] * 100).round(1)\n",
    "\n",
    "# 7. Features de r√©cup√©ration (jours depuis derni√®re session)\n",
    "df['Jours_Repos'] = df.groupby('Exercice')['Date'].diff().dt.days\n",
    "df['Jours_Repos'] = df['Jours_Repos'].fillna(0)  # Premier entra√Ænement = 0 jours\n",
    "\n",
    "# 8. Classification des zones d'intensit√©\n",
    "def classify_intensity_zone(intensity):\n",
    "    \"\"\"Classifie les zones d'intensit√© bas√©es sur % 1RM\"\"\"\n",
    "    if pd.isna(intensity):\n",
    "        return 'Unknown'\n",
    "    elif intensity < 60:\n",
    "        return 'Light'  # √âchauffement/r√©cup√©ration\n",
    "    elif intensity < 75:\n",
    "        return 'Moderate'  # Endurance/volume\n",
    "    elif intensity < 85:\n",
    "        return 'Heavy'  # Force\n",
    "    else:\n",
    "        return 'Max'  # Maximal\n",
    "\n",
    "df['Zone_Intensite'] = df['Intensite_Relative'].apply(classify_intensity_zone)\n",
    "\n",
    "print(\"‚úÖ Features de performance calcul√©es:\")\n",
    "performance_features = ['Intensite_Relative', 'Volume_Par_Kg_Corps', 'Densite_Volume',\n",
    "                       'Fatigue_Index', 'Efficacite_Volume', 'Ecart_Moyenne_Perso',\n",
    "                       'Jours_Repos', 'Zone_Intensite']\n",
    "\n",
    "for feature in performance_features:\n",
    "    print(f\"   ‚Ä¢ {feature}\")\n",
    "\n",
    "print(f\"\\nüìä Distribution des zones d'intensit√©:\")\n",
    "zone_counts = df['Zone_Intensite'].value_counts()\n",
    "print(zone_counts)\n",
    "print(f\"\\nPourcentages:\")\n",
    "print((zone_counts / len(df) * 100).round(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a56edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des features de performance\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('üéØ Analyse des Features de Performance', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Distribution de l'intensit√© relative\n",
    "df['Intensite_Relative'].hist(bins=20, ax=axes[0,0], alpha=0.7, color='lightblue', edgecolor='black')\n",
    "axes[0,0].set_title('Distribution Intensit√© Relative (% 1RM)')\n",
    "axes[0,0].set_xlabel('Intensit√© (%)')\n",
    "axes[0,0].set_ylabel('Fr√©quence')\n",
    "axes[0,0].axvline(df['Intensite_Relative'].mean(), color='red', linestyle='--', \n",
    "                 label=f'Moyenne: {df[\"Intensite_Relative\"].mean():.1f}%')\n",
    "axes[0,0].legend()\n",
    "\n",
    "# 2. Zones d'intensit√©\n",
    "zone_counts.plot(kind='bar', ax=axes[0,1], color='steelblue', alpha=0.8)\n",
    "axes[0,1].set_title('R√©partition par Zone d\\'Intensit√©')\n",
    "axes[0,1].set_ylabel('Nombre de Sets')\n",
    "axes[0,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 3. Fatigue index par exercice\n",
    "fatigue_by_exercise = df.groupby('Exercice')['Fatigue_Index'].mean()\n",
    "fatigue_by_exercise.plot(kind='bar', ax=axes[0,2], color='orange', alpha=0.8)\n",
    "axes[0,2].set_title('Fatigue Index Moyen par Exercice')\n",
    "axes[0,2].set_ylabel('Fatigue Index (%)')\n",
    "axes[0,2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 4. Relation intensit√© vs volume\n",
    "axes[1,0].scatter(df['Intensite_Relative'], df['Volume'], alpha=0.6, color='green')\n",
    "axes[1,0].set_title('Relation Intensit√© vs Volume')\n",
    "axes[1,0].set_xlabel('Intensit√© Relative (%)')\n",
    "axes[1,0].set_ylabel('Volume (kg)')\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Jours de repos vs performance\n",
    "rest_performance = df.groupby('Jours_Repos')['Intensite_Relative'].mean()\n",
    "rest_performance.plot(kind='line', marker='o', ax=axes[1,1], color='purple')\n",
    "axes[1,1].set_title('Jours de Repos vs Performance')\n",
    "axes[1,1].set_xlabel('Jours de Repos')\n",
    "axes[1,1].set_ylabel('Intensit√© Moyenne (%)')\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Efficacit√© par exercice\n",
    "efficacite_by_exercise = df.groupby('Exercice')['Efficacite_Volume'].mean()\n",
    "efficacite_by_exercise.plot(kind='bar', ax=axes[1,2], color='darkred', alpha=0.8)\n",
    "axes[1,2].set_title('Efficacit√© Volume par Exercice')\n",
    "axes[1,2].set_ylabel('Efficacit√© (kg/rep)')\n",
    "axes[1,2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca84f7c0",
   "metadata": {},
   "source": [
    "## üßÆ Features temporelles et cycliques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbebb209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features temporelles et cycliques\n",
    "print(\"üßÆ FEATURES TEMPORELLES ET CYCLIQUES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. Features cycliques pour jours de la semaine\n",
    "df['Jour_Sin'] = np.sin(2 * np.pi * df['Jour_Numero'] / 7)\n",
    "df['Jour_Cos'] = np.cos(2 * np.pi * df['Jour_Numero'] / 7)\n",
    "\n",
    "# 2. Features cycliques pour mois\n",
    "df['Mois_Sin'] = np.sin(2 * np.pi * df['Mois'] / 12)\n",
    "df['Mois_Cos'] = np.cos(2 * np.pi * df['Mois'] / 12)\n",
    "\n",
    "# 3. Timestamp normalis√© (progression temporelle)\n",
    "min_date = df['Date'].min()\n",
    "max_date = df['Date'].max()\n",
    "df['Timestamp_Norm'] = ((df['Date'] - min_date) / (max_date - min_date)).round(4)\n",
    "\n",
    "# 4. Features de s√©rie dans la session\n",
    "df['Set_Number_Session'] = df.groupby(['Date', 'Exercice']).cumcount() + 1\n",
    "df['Total_Sets_Session'] = df.groupby(['Date', 'Exercice'])['Set_Number_Session'].transform('max')\n",
    "df['Pct_Session_Progress'] = (df['Set_Number_Session'] / df['Total_Sets_Session'] * 100).round(1)\n",
    "\n",
    "# 5. Cumuls temporels\n",
    "df['Volume_Cumule_Total'] = df.groupby('Exercice')['Volume'].cumsum()\n",
    "df['Sessions_Cumule'] = df.groupby('Exercice').cumcount() + 1\n",
    "\n",
    "# 6. Momentum (acc√©l√©ration des progressions)\n",
    "df['Momentum_Poids'] = df.groupby('Exercice')['Progression_Poids'].rolling(window=3, min_periods=1).mean().reset_index(0, drop=True)\n",
    "df['Momentum_Volume'] = df.groupby('Exercice')['Progression_Volume'].rolling(window=3, min_periods=1).mean().reset_index(0, drop=True)\n",
    "\n",
    "# 7. Ratios et proportions\n",
    "# Volume par session (agr√©g√© par jour)\n",
    "daily_volume = df.groupby(['Date', 'Exercice'])['Volume'].sum().reset_index()\n",
    "daily_volume.columns = ['Date', 'Exercice', 'Volume_Session']\n",
    "df = df.merge(daily_volume, on=['Date', 'Exercice'], how='left')\n",
    "\n",
    "df['Pct_Volume_Session'] = (df['Volume'] / df['Volume_Session'] * 100).round(1)\n",
    "\n",
    "print(\"‚úÖ Features temporelles et cycliques calcul√©es:\")\n",
    "temporal_features = ['Jour_Sin', 'Jour_Cos', 'Mois_Sin', 'Mois_Cos', 'Timestamp_Norm',\n",
    "                    'Set_Number_Session', 'Pct_Session_Progress', 'Volume_Cumule_Total',\n",
    "                    'Sessions_Cumule', 'Momentum_Poids', 'Momentum_Volume', 'Pct_Volume_Session']\n",
    "\n",
    "for feature in temporal_features:\n",
    "    print(f\"   ‚Ä¢ {feature}\")\n",
    "\n",
    "print(f\"\\nüìä Aper√ßu des features temporelles:\")\n",
    "temporal_sample = df[['Date', 'Exercice'] + temporal_features[:6]].head(8)\n",
    "print(temporal_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdf7096",
   "metadata": {},
   "source": [
    "## üìä Analyse de corr√©lation et importance des features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c50282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse de corr√©lation des nouvelles features\n",
    "print(\"üìä ANALYSE DE CORR√âLATION ET IMPORTANCE DES FEATURES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# S√©lection des features num√©riques pour l'analyse\n",
    "numeric_features = ['Poids_kg', 'Reps', 'Volume', '1RM_Moyen', 'Session_Number',\n",
    "                   'Intensite_Relative', 'Fatigue_Index', 'Efficacite_Volume',\n",
    "                   'Jours_Repos', 'Progression_Poids', 'Progression_Volume',\n",
    "                   'Pct_Max_Poids', 'Volume_Cumule_Total', 'Momentum_Poids']\n",
    "\n",
    "# Filtrer les features qui existent dans le DataFrame\n",
    "existing_features = [f for f in numeric_features if f in df.columns]\n",
    "correlation_matrix = df[existing_features].corr()\n",
    "\n",
    "# Visualisation de la matrice de corr√©lation\n",
    "plt.figure(figsize=(14, 12))\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='RdBu_r', center=0,\n",
    "            square=True, fmt='.2f', cbar_kws={\"shrink\": .8})\n",
    "plt.title('üîç Matrice de Corr√©lation des Features Principales', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identification des corr√©lations fortes\n",
    "print(\"\\nüîó Corr√©lations significatives (|r| > 0.7):\")\n",
    "high_corr_pairs = []\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        corr_value = correlation_matrix.iloc[i, j]\n",
    "        if abs(corr_value) > 0.7:\n",
    "            feature1 = correlation_matrix.columns[i]\n",
    "            feature2 = correlation_matrix.columns[j]\n",
    "            high_corr_pairs.append((feature1, feature2, corr_value))\n",
    "            print(f\"   ‚Ä¢ {feature1} ‚Üî {feature2}: {corr_value:.3f}\")\n",
    "\n",
    "if not high_corr_pairs:\n",
    "    print(\"   Aucune corr√©lation > 0.7 d√©tect√©e\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6636ff23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse de l'importance des features avec Information Mutuelle\n",
    "print(\"\\nüéØ IMPORTANCE DES FEATURES (INFORMATION MUTUELLE)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Pr√©paration des donn√©es pour l'analyse d'importance\n",
    "# Target: progression du poids (binaire - progression vs non-progression)\n",
    "df_analysis = df.dropna(subset=['Progression_Poids']).copy()\n",
    "df_analysis['Target_Progression'] = (df_analysis['Progression_Poids'] > 0).astype(int)\n",
    "\n",
    "# Features pour l'analyse d'importance\n",
    "feature_cols = ['Session_Number', 'Intensite_Relative', 'Jours_Repos', \n",
    "               'Fatigue_Index', 'Pct_Max_Poids', 'Volume_Cumule_Total',\n",
    "               'Momentum_Poids', 'Set_Number_Session', 'Jour_Sin', 'Jour_Cos']\n",
    "\n",
    "# Filtrer les features existantes et sans NaN\n",
    "available_features = []\n",
    "for col in feature_cols:\n",
    "    if col in df_analysis.columns and not df_analysis[col].isna().all():\n",
    "        available_features.append(col)\n",
    "\n",
    "if len(available_features) > 0 and len(df_analysis) > 10:\n",
    "    X = df_analysis[available_features].fillna(0)\n",
    "    y = df_analysis['Target_Progression']\n",
    "    \n",
    "    # Calcul de l'information mutuelle\n",
    "    mi_scores = mutual_info_regression(X, y, random_state=42)\n",
    "    \n",
    "    # Cr√©ation du DataFrame des scores\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': available_features,\n",
    "        'Importance_Score': mi_scores\n",
    "    }).sort_values('Importance_Score', ascending=False)\n",
    "    \n",
    "    print(\"üìä Importance des features pour pr√©dire la progression:\")\n",
    "    print(importance_df)\n",
    "    \n",
    "    # Visualisation de l'importance\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(importance_df['Feature'], importance_df['Importance_Score'], \n",
    "            color='steelblue', alpha=0.8)\n",
    "    plt.title('üéØ Importance des Features pour Pr√©dire la Progression', \n",
    "              fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Score d\\'Importance (Information Mutuelle)')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nüèÜ Top 3 des features les plus importantes:\")\n",
    "    for i in range(min(3, len(importance_df))):\n",
    "        feature = importance_df.iloc[i]['Feature']\n",
    "        score = importance_df.iloc[i]['Importance_Score']\n",
    "        print(f\"   {i+1}. {feature}: {score:.4f}\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Pas assez de donn√©es pour l'analyse d'importance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea322c3",
   "metadata": {},
   "source": [
    "## üíæ Sauvegarde et r√©sum√© final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1d5f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R√©sum√© final de toutes les features cr√©√©es\n",
    "print(\"üíæ R√âSUM√â FINAL DES FEATURES CR√â√âES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Comptage des features par cat√©gorie\n",
    "original_features = ['Date', 'Exercice', 'Poids_kg', 'Reps', 'Volume', 'Type_serie', 'R√©gion']\n",
    "oneRM_features = ['1RM_Epley', '1RM_Brzycki', '1RM_Lombardi', '1RM_Moyen']\n",
    "progression_features_final = ['Session_Number', 'Max_Poids_Personnel', 'Max_1RM_Personnel', \n",
    "                             'Pct_Max_Poids', 'Pct_Max_1RM', 'Progression_Poids', \n",
    "                             'Progression_Volume', 'Progression_1RM']\n",
    "performance_features_final = ['Intensite_Relative', 'Volume_Par_Kg_Corps', 'Densite_Volume',\n",
    "                             'Fatigue_Index', 'Efficacite_Volume', 'Ecart_Moyenne_Perso',\n",
    "                             'Jours_Repos', 'Zone_Intensite']\n",
    "temporal_features_final = ['Jour_Sin', 'Jour_Cos', 'Mois_Sin', 'Mois_Cos', 'Timestamp_Norm',\n",
    "                          'Set_Number_Session', 'Pct_Session_Progress', 'Volume_Cumule_Total',\n",
    "                          'Sessions_Cumule', 'Momentum_Poids', 'Momentum_Volume']\n",
    "\n",
    "# Rolling features\n",
    "rolling_features = [col for col in df.columns if 'Rolling' in col]\n",
    "trend_features = [col for col in df.columns if 'Tendance' in col]\n",
    "\n",
    "print(f\"üìä INVENTAIRE DES FEATURES:\")\n",
    "print(f\"   ‚Ä¢ Features originales: {len(original_features)}\")\n",
    "print(f\"   ‚Ä¢ Features 1RM: {len(oneRM_features)}\")\n",
    "print(f\"   ‚Ä¢ Features progression: {len(progression_features_final)}\")\n",
    "print(f\"   ‚Ä¢ Features performance: {len(performance_features_final)}\")\n",
    "print(f\"   ‚Ä¢ Features temporelles: {len(temporal_features_final)}\")\n",
    "print(f\"   ‚Ä¢ Features rolling: {len(rolling_features)}\")\n",
    "print(f\"   ‚Ä¢ Features tendance: {len(trend_features)}\")\n",
    "\n",
    "total_features = len(df.columns)\n",
    "new_features_count = total_features - len(df_raw.columns)\n",
    "print(f\"\\nüéØ TOTAL:\")\n",
    "print(f\"   ‚Ä¢ Features totales: {total_features}\")\n",
    "print(f\"   ‚Ä¢ Nouvelles features cr√©√©es: {new_features_count}\")\n",
    "\n",
    "# Qualit√© des donn√©es finales\n",
    "print(f\"\\n‚úÖ QUALIT√â DES DONN√âES FINALES:\")\n",
    "missing_pct = (df.isnull().sum().sum() / (len(df) * len(df.columns)) * 100)\n",
    "print(f\"   ‚Ä¢ Compl√©tude globale: {100 - missing_pct:.1f}%\")\n",
    "print(f\"   ‚Ä¢ Lignes de donn√©es: {len(df)}\")\n",
    "print(f\"   ‚Ä¢ Exercices couverts: {df['Exercice'].nunique()}\")\n",
    "print(f\"   ‚Ä¢ P√©riode couverte: {(df['Date'].max() - df['Date'].min()).days} jours\")\n",
    "\n",
    "# Features avec le plus de valeurs manquantes\n",
    "missing_by_feature = df.isnull().sum().sort_values(ascending=False)\n",
    "if missing_by_feature.iloc[0] > 0:\n",
    "    print(f\"\\n‚ö†Ô∏è Features avec valeurs manquantes:\")\n",
    "    for feature, missing_count in missing_by_feature.head(5).items():\n",
    "        if missing_count > 0:\n",
    "            pct = missing_count / len(df) * 100\n",
    "            print(f\"   ‚Ä¢ {feature}: {missing_count} ({pct:.1f}%)\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ Aucune valeur manquante d√©tect√©e\")\n",
    "\n",
    "print(f\"\\nüöÄ PR√äT POUR:\")\n",
    "print(f\"   ‚úÖ Mod√©lisation ML (pr√©diction de progression)\")\n",
    "print(f\"   ‚úÖ Analyse de clustering (profils d'entra√Ænement)\")\n",
    "print(f\"   ‚úÖ D√©tection d'anomalies (performances inhabituelles)\")\n",
    "print(f\"   ‚úÖ Recommandations personnalis√©es\")\n",
    "print(f\"   ‚úÖ Dashboard avanc√© avec m√©triques enrichies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb810a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde optionnelle du dataset enrichi\n",
    "print(\"üíæ SAUVEGARDE DU DATASET ENRICHI\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# S√©lection des features les plus importantes pour la sauvegarde\n",
    "essential_features = [\n",
    "    # Features de base\n",
    "    'Date', 'Exercice', 'R√©gion', 'Poids_kg', 'Reps', 'Volume', 'Type_serie',\n",
    "    # Features 1RM\n",
    "    '1RM_Moyen', 'Intensite_Relative',\n",
    "    # Features progression\n",
    "    'Session_Number', 'Max_Poids_Personnel', 'Pct_Max_Poids', 'Progression_Poids',\n",
    "    # Features performance\n",
    "    'Fatigue_Index', 'Efficacite_Volume', 'Jours_Repos', 'Zone_Intensite',\n",
    "    # Features temporelles\n",
    "    'Volume_Cumule_Total', 'Momentum_Poids', 'Set_Number_Session'\n",
    "]\n",
    "\n",
    "# Filtrer les features qui existent\n",
    "available_essential = [f for f in essential_features if f in df.columns]\n",
    "df_export = df[available_essential].copy()\n",
    "\n",
    "# Informations sur l'export\n",
    "print(f\"üìä Dataset d'export pr√©par√©:\")\n",
    "print(f\"   ‚Ä¢ Features s√©lectionn√©es: {len(available_essential)}\")\n",
    "print(f\"   ‚Ä¢ Lignes: {len(df_export)}\")\n",
    "print(f\"   ‚Ä¢ Taille estim√©e: {df_export.memory_usage(deep=True).sum() / 1024:.1f} KB\")\n",
    "\n",
    "print(f\"\\nüìã Features incluses dans l'export:\")\n",
    "for i, feature in enumerate(available_essential, 1):\n",
    "    print(f\"   {i:2d}. {feature}\")\n",
    "\n",
    "# Aper√ßu du dataset final\n",
    "print(f\"\\nüîç Aper√ßu du dataset enrichi:\")\n",
    "print(df_export.head())\n",
    "\n",
    "print(f\"\\n‚úÖ Dataset pr√™t pour export vers:\")\n",
    "print(f\"   ‚Ä¢ CSV: ../data/features_enriched.csv\")\n",
    "print(f\"   ‚Ä¢ Base de donn√©es pour API\")\n",
    "print(f\"   ‚Ä¢ Notebooks de mod√©lisation ML\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24acc6b7",
   "metadata": {},
   "source": [
    "---\n",
    "## üìù R√©sum√© du Feature Engineering\n",
    "\n",
    "Ce notebook a cr√©√© un ensemble complet de features avanc√©es pour l'analyse de musculation :\n",
    "\n",
    "### ‚úÖ Features cr√©√©es\n",
    "- **üí™ 1RM & Intensit√©** : 4 formules de calcul + zones d'intensit√©\n",
    "- **üìà Progression** : Tendances, maximums personnels, pourcentages\n",
    "- **üéØ Performance** : Fatigue, efficacit√©, densit√©, r√©cup√©ration\n",
    "- **‚è±Ô∏è Temporelles** : Cycliques, cumuls, momentum\n",
    "- **üîÑ Rolling** : Moyennes mobiles sur diff√©rentes fen√™tres\n",
    "\n",
    "### üéØ Insights cl√©s\n",
    "- **Nombreuses nouvelles features** g√©n√©r√©es automatiquement\n",
    "- **Corr√©lations analys√©es** pour √©viter la redondance\n",
    "- **Importance √©valu√©e** pour la pr√©diction de progression\n",
    "- **Qualit√© valid√©e** avec gestion des valeurs manquantes\n",
    "\n",
    "### üìä Applications possibles\n",
    "- **Pr√©diction de performance** future\n",
    "- **D√©tection de plateaux** automatis√©e\n",
    "- **Recommandations personnalis√©es** de charge\n",
    "- **Clustering** de profils d'entra√Ænement\n",
    "- **Dashboard enrichi** avec m√©triques avanc√©es\n",
    "\n",
    "**Dataset pr√™t pour la Phase 3 (Mod√®les ML) et les phases suivantes !**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
