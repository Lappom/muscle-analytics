#!/usr/bin/env python3
"""
Script de d√©monstration pour le pipeline ETL complet.

Ce script d√©montre l'utilisation du pipeline ETL de bout en bout :
- Traitement des fichiers CSV/XML
- Insertion en base de donn√©es
- Import incr√©mental
- G√©n√©ration de rapports
"""

import logging
import sys
from pathlib import Path

# Ajout du chemin src pour les imports
sys.path.insert(0, str(Path(__file__).parent.parent / 'src'))

from etl.pipeline import ETLPipeline
from src.database import DatabaseManager
from etl.import_scripts import ETLImporter
from config.database import get_db_config

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('etl_demo.log', encoding='utf-8')
    ]
)

logger = logging.getLogger(__name__)


def test_database_connection():
    """Test la connexion √† la base de donn√©es"""
    logger.info("=== TEST DE CONNEXION √Ä LA BASE DE DONN√âES ===")
    
    # Utilisation de la configuration automatique (d√©tecte l'environnement)
    db_config = get_db_config()
    db_manager = DatabaseManager(**db_config)
    
    if db_manager.test_connection():
        logger.info("CONNEXION R√âUSSIE √† la base de donn√©es")
        
        # Affichage des statistiques
        try:
            stats = db_manager.get_database_stats()
            logger.info(f"Statistiques actuelles:")
            logger.info(f"   - Sessions: {stats['total_sessions']}")
            logger.info(f"   - S√©ries: {stats['total_sets']}")
            logger.info(f"   - Exercices uniques: {stats['unique_exercises']}")
            logger.info(f"   - Exercices catalogu√©s: {stats['catalogued_exercises']}")
            
            if stats['date_range']['start'] and stats['date_range']['end']:
                logger.info(f"   - P√©riode: {stats['date_range']['start']} √† {stats['date_range']['end']}")
        except Exception as e:
            logger.warning(f"Impossible de r√©cup√©rer les statistiques: {e}")
        
        return True
    else:
        logger.error("√âCHEC de la connexion √† la base de donn√©es")
        logger.error("V√©rifiez que PostgreSQL est d√©marr√© et que la base existe")
        return False


def test_pipeline_processing():
    """Test le traitement des fichiers d'exemple"""
    logger.info("=== TEST DU PIPELINE DE TRAITEMENT ===")
    
    pipeline = ETLPipeline()
    examples_dir = Path(__file__).parent
    
    # Test du fichier CSV
    csv_file = examples_dir / 'sample_data.csv'
    if csv_file.exists():
        logger.info(f"Traitement du fichier CSV: {csv_file.name}")
        try:
            df_csv = pipeline.process_file(csv_file)
            logger.info(f"CSV trait√© AVEC SUCC√àS: {len(df_csv)} lignes normalis√©es")
            
            # Validation de la qualit√©
            quality = pipeline.validate_data_quality(df_csv)
            logger.info(f"Qualit√©: {quality['quality_percentage']:.1f}% ({quality['valid_sets']}/{quality['total_rows']} s√©ries valides)")
            
        except Exception as e:
            logger.error(f"ERREUR CSV: {e}")
    else:
        logger.warning(f"Fichier CSV non trouv√©: {csv_file}")
    
    # Test du fichier XML
    xml_file = examples_dir / 'sample_data.xml'
    if xml_file.exists():
        logger.info(f"Traitement du fichier XML: {xml_file.name}")
        try:
            df_xml = pipeline.process_file(xml_file)
            logger.info(f"XML trait√© AVEC SUCC√àS: {len(df_xml)} lignes normalis√©es")
            
            # Validation de la qualit√©
            quality = pipeline.validate_data_quality(df_xml)
            logger.info(f"Qualit√©: {quality['quality_percentage']:.1f}% ({quality['valid_sets']}/{quality['total_rows']} s√©ries valides)")
            
        except Exception as e:
            logger.error(f"ERREUR XML: {e}")
    else:
        logger.warning(f"Fichier XML non trouv√©: {xml_file}")


def test_database_import():
    """Test l'import en base de donn√©es"""
    logger.info("=== TEST D'IMPORT EN BASE DE DONN√âES ===")
    
    # Utilisation de la configuration automatique
    db_config = get_db_config()
    db_manager = DatabaseManager(**db_config)
    importer = ETLImporter(db_manager)
    examples_dir = Path(__file__).parent
    
    # Test d'import d'un fichier
    csv_file = examples_dir / 'sample_data.csv'
    if csv_file.exists():
        logger.info(f"Import du fichier: {csv_file.name}")
        try:
            result = importer.import_file(csv_file, force_import=True)
            
            if result['success']:
                logger.info(f"Import R√âUSSI: {result['message']}")
                logger.info(f"Statistiques: {result['stats']}")
            else:
                logger.error(f"√âCHEC d'import: {result['message']}")
            
            # G√©n√©ration du rapport
            report = importer.generate_import_report(result)
            logger.info(f"Rapport d'import:\n{report}")
            
        except Exception as e:
            logger.error(f"ERREUR d'import: {e}")
    
    # Test d'import de r√©pertoire
    if examples_dir.exists():
        logger.info(f"Import du r√©pertoire: {examples_dir}")
        try:
            result = importer.import_directory(examples_dir, force_import=False)
            
            if result['success']:
                logger.info(f"‚úÖ Import r√©pertoire r√©ussi: {result['message']}")
            else:
                logger.info(f"‚ÑπÔ∏è  Import r√©pertoire: {result['message']}")
            
            # G√©n√©ration du rapport
            report = importer.generate_import_report(result)
            logger.info(f"üìÑ Rapport d'import r√©pertoire:\n{report}")
            
        except Exception as e:
            logger.error(f"‚ùå Erreur d'import r√©pertoire: {e}")


def test_incremental_import():
    """Test l'import incr√©mental"""
    logger.info("=== TEST D'IMPORT INCR√âMENTAL ===")
    
    # Utilisation de la configuration automatique
    db_config = get_db_config()
    db_manager = DatabaseManager(**db_config)
    importer = ETLImporter(db_manager)
    examples_dir = Path(__file__).parent
    
    if examples_dir.exists():
        logger.info(f"Import incr√©mental du r√©pertoire: {examples_dir}")
        try:
            result = importer.incremental_import(examples_dir, days_threshold=30)
            
            if result['success']:
                logger.info(f"‚úÖ Import incr√©mental r√©ussi: {result['message']}")
            else:
                logger.info(f"‚ÑπÔ∏è  Import incr√©mental: {result['message']}")
            
            logger.info(f"üìä Statistiques: {result['stats']}")
            
        except Exception as e:
            logger.error(f"‚ùå Erreur d'import incr√©mental: {e}")


def generate_demo_report():
    """G√©n√®re un rapport de d√©monstration complet"""
    logger.info("=== G√âN√âRATION DU RAPPORT DE D√âMONSTRATION ===")
    
    try:
        pipeline = ETLPipeline()
        # Utilisation de la configuration automatique
        db_config = get_db_config()
        db_manager = DatabaseManager(**db_config)
        
        # Statistiques finales de la base
        if db_manager.test_connection():
            stats = db_manager.get_database_stats()
            
            logger.info("üìä RAPPORT FINAL:")
            logger.info(f"   - Sessions totales: {stats['total_sessions']}")
            logger.info(f"   - S√©ries totales: {stats['total_sets']}")
            logger.info(f"   - Exercices uniques: {stats['unique_exercises']}")
            logger.info(f"   - Exercices catalogu√©s: {stats['catalogued_exercises']}")
            
            if stats['date_range']['start'] and stats['date_range']['end']:
                logger.info(f"   - P√©riode couverte: {stats['date_range']['start']} √† {stats['date_range']['end']}")
        
        # Test de g√©n√©ration de rapport de synth√®se
        examples_dir = Path(__file__).parent
        csv_file = examples_dir / 'sample_data.csv'
        
        if csv_file.exists():
            df = pipeline.process_file(csv_file)
            if not df.empty:
                summary_report = pipeline.generate_summary_report(df)
                logger.info(f"üìÑ RAPPORT DE SYNTH√àSE DES DONN√âES:\n{summary_report}")
        
    except Exception as e:
        logger.error(f"‚ùå Erreur lors de la g√©n√©ration du rapport: {e}")


def main():
    """Fonction principale de d√©monstration"""
    logger.info("D√âMONSTRATION DU PIPELINE ETL - MUSCLE ANALYTICS")
    logger.info("=" * 60)
    
    try:
        # 1. Test de connexion
        if not test_database_connection():
            logger.error("ARR√äT de la d√©monstration - probl√®me de base de donn√©es")
            return
        
        logger.info('')  # Ligne vide pour la lisibilit√©
        
        # 2. Test du pipeline
        test_pipeline_processing()
        logger.info('')
        
        # 3. Test d'import
        test_database_import()
        logger.info('')
        
        # 4. Test d'import incr√©mental
        test_incremental_import()
        logger.info('')
        
        # 5. Rapport final
        generate_demo_report()
        
        logger.info("D√âMONSTRATION TERMIN√âE AVEC SUCC√àS")
        
    except KeyboardInterrupt:
        logger.info("D√©monstration interrompue par l'utilisateur")
    except Exception as e:
        logger.error(f"Erreur inattendue: {e}")
        raise


if __name__ == "__main__":
    main()
